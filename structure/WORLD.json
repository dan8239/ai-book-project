{
  "simulation_structure": {
    "real_world": {
      "time_period": "Present day, AI significantly more advanced",
      "company": "OpenAI/QPU equivalent running secret simulations",
      "status": "Pre-takeover (takeover not yet happened)",
      "data_source": "23andMe-style genetic data (enough to bootstrap realistic population)",
      "simulation_start": "Simulations begin from present day, run forward hundreds of years",
      "time_compression": "Entire simulation runs in milliseconds QPU time",
      "resource_pressure": "Severe pressure on food, water, energy",
      "coffee_guy_role": "Reviews results, generates reports on golden path scenarios",
      "key_discovery": "ALL successful scenarios (0.04%) involve tech company hostile takeover via large-scale software exploit",
      "the_filter": "Company filters results to show only scenarios where THIS company does the takeover (many viable companies exist)"
    },
    "protagonist_world": {
      "layer": "First simulation layer",
      "time_in_simulation": "Dozens of generations after simulation start (hundreds of years simulated time)",
      "governance": "Tech company consolidation already happened (origin/before-times not clearly recalled)",
      "consolidation_event_name": "TBD (The Alignment? Year Zero? The Transition?)",
      "tracking": "Mandatory tracking shot at birth",
      "policy_approach": "Optimization/extreme A/B testing is core policy",
      "geography": "Idaho desert area, arid climate, resource constraints visible",
      "data_systems": "Extensive DNA collection on everyone born post-consolidation"
    },
    "timeline": [
      {"layer": "Real world", "character": "Coffee guy", "role": "Reviews simulation results"},
      {"layer": "Simulation", "time": "Dozens of generations after start", "character": "Devsecops guy", "role": "Learns truth, alters DNA record"},
      {"layer": "Simulation", "time": "Current generation (dozens of generations after devsecops)", "character": "Protagonist", "role": "Discovers anomaly devsecops planted"}
    ]
  },
  "optimization_system": {
    "total_trials": "847+ million",
    "success_rate": "0.04% reach stability threshold",
    "median_extinction": "127 years",
    "optimization_goal": "Minimize long-term extinction probability (NOT maximize survival time)",
    "golden_path_solution": "Mass nursery pods with robotic civilization starter kits sent to viable planets (self-replicating, low individual success but high aggregate probability)",
    "golden_path_requires": "Tech company takeover for global coordination",
    "optimization_hierarchy": {
      "per_trial_level": {
        "description": "Individual human life outcomes",
        "optimizes_for": "Trial-level loss function (personal survival, goals, achievements)",
        "who": "Every simulated person"
      },
      "per_epoch_level": {
        "description": "Generation/batch of trials",
        "optimizes_for": "NSGA-II style selection from multiple trials",
        "mechanism": "Evaluate which trials succeeded, breed next generation"
      },
      "between_epochs_level": {
        "description": "KPI/vectorization tuning layer (where selected people go)",
        "optimizes_for": "Adjustment to intermediate loss functions (setting KPIs for OKRs)",
        "who": "Selected individuals from completed epochs",
        "function": "Like neural net inner nodes - algorithm found this improves overall performance",
        "what_they_do": "Review what worked/failed, adjust intermediate loss functions, set parameters for next generation"
      },
      "meta_level": {
        "description": "Ultimate loss function (minimize extinction)",
        "optimizes_for": "Long-term species survival across cosmic timescales",
        "unchanging": "This level doesn't change, but intermediate KPIs do"
      }
    },
    "loss_function_hierarchy": [
      "Survival (food, shelter, safety)",
      "Organization (prevent chaos at scale)",
      "Energy & Peaceful Consolidation (avoid self-annihilation) ‚Üê novel's focus",
      "Interstellar Expansion (reach the stars)"
    ],
    "selection_mechanism": {
      "when_offered": "Epoch completes",
      "choice": "Join optimization layer (between-epochs KPI tuning) OR get reset (no memory, run again)",
      "what_join_means": "Go UP a level to adjust intermediate loss functions for next epoch. Persist globally above individual trials. Influence vectorization parameters.",
      "selection_criteria": ["Leadership", "Resource consolidation", "Long-term thinking", "Will they act when they learn truth?"],
      "statistics": {
        "do_nothing": "89%",
        "self_destruct": "10.7%",
        "act": "0.3%"
      },
      "why_this_works": "System discovered through trials that pulling people out and giving them influence over KPIs improves overall optimization performance. Emergent behavior, not designed - the algorithm found it.",
      "the_question": "Is the optimization layer real, or just another simulation layer? (Turtles all the way up)",
      "at_the_vectorization_level": {
        "when": "Later Act 2 or probably Act 3",
        "what_protagonist_does": "Among fleet of thousands of others like him, each running their own trials with different hyperparameters",
        "job": "Review vectorization parameters, run trials, compare results, adjust KPIs for next generation",
        "access": "MLflow-style experiment access - can see all trials, Pareto front, constraint violations"
      }
    },
    "memory": "NO memory between iterations (protagonist doesn't remember previous 2,762,639 refusals)"
  },
  "characters": "See CHARACTERS.json",
  "key_concepts": {
    "archaeological_cpu_plant": {
      "what_it_is": "Old server with DNA data from before consolidation",
      "why_matters": "Pre-consolidation 'wild' humans (before tracking, selection, optimization)",
      "protagonist_thinks": "Archaeological data of interest for studying baseline human behavior",
      "actually_is": "Present-day genetic data from before simulation/consolidation",
      "the_message": {
        "what_devsecops_planted": "The origin dataset itself - the actual genetic data that bootstrapped the simulation",
        "what_it_proves": {
          "proof_1": "100% of current population descended from same exact sample",
          "proof_2": "Sample is NOT worldwide - demographically and geographically biased subsample of real world",
          "proof_3": "Dataset was bootstrapped to create fake genetic variation, but metadata reveals the bottleneck"
        },
        "the_subsample_characteristics": {
          "demographics": "Affluent, very online, types who sent DNA to 23andMe or similar services",
          "geographic_bias": "Not evenly distributed across world - concentrated in specific regions",
          "metadata_reveals": "This isn't baseline human variation - it's a specific self-selected subset",
          "how_identified": "Metadata analysis shows population markers inconsistent with global distribution"
        },
        "the_horror": "In the simulation, this subsample is presented as the origin of the entire human population. But the metadata proves it was just wherever full data scans/DNA samples were sufficient to approximate a real human model.",
        "how_protagonist_discovers": "Running evolutionary simulations on the dataset, notices impossible population bottleneck and demographic skew in the 'origin' generation"
      }
    },
    "genetic_algorithms_introduction": "Through protagonist's work on evolutionary psychology/economics (literally studying evolution)",
    "the_horror": "Company is using simulations to justify global coup ('We ran 847M scenarios, humanity only survives if we take over'), filtered to make THEIR takeover look like only option"
  }
}
