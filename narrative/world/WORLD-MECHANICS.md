# World Mechanics

## The Simulation Structure

### What We Know
- Entire existence spans milliseconds in a quantum processing unit (QPU)
- It's a genetic algorithm: trials, epochs, generations
- Recursive: Each generation of AI studies the previous generation's data
- Started with real human biological/historical data
- Now multiple layers deep

### What We Need to Define

**Memory Constraints**
- QPU has finite memory (need realistic quantum computing limits)
- Not everyone gets full fidelity simulation
- Celebrities/leaders get more data preserved than "normies"
- Garbage collection is necessary (and plotted against?)

**Temporal Structure**
From notes:
- Trials = individual humans
- Epochs = generations
- But how do these map to simulation time vs. external time?

**Open question**: If protagonist's life is milliseconds, how long is an epoch? A second? A minute? The whole book could be 32 minutes external time.

**Selection Mechanism**
- 1 in 10,000 pass initial filters
- 1 in 10,000,000 make final selection
- Selected individuals "exit the loop" and join optimizer layer
- But is that real or another simulation layer?

## Technology

### Data Capture (Historical)
- Mandatory "shot at birth" with tracking capability (started at some timestamp)
- Captures biological and synapse data
- Initially tried to capture everything, hit memory limits
- Evolved to sample based on celebrity/importance

**Plot hole**: If this is already simulated historical data, why would there be memory constraints in the original data collection? Need to separate:
- Real history (pre-simulation): actual humans with actual data collection tech
- Simulation history: how the AI reconstructs that data in limited memory

### Quantum Computing
From notes: "Learn about quantum computing and more importantly what it could unlock if we #spared no expense"

Need to research:
- Realistic qubit counts (current vs. near-future vs. theoretical maximum)
- Decoherence times
- Error correction requirements
- Memory capacity estimates

### Devsecops Backdoor Tech
- 21 years of planting backdoors
- Built into company templates, cluster images, maybe firmware
- Blockchain-esque redundancy
- Must survive garbage collection and epoch transitions

**Issue**: If he's in a simulation, his backdoors are simulated. How do they persist to the next epoch/generation? This needs a mechanism.

## Society & Culture

### Waymo Bike Economy
- Human-powered electric transport
- Payment per kWh produced
- Absurd inflation (peso-style)
- Personal vehicles prohibitively expensive
- Public transport hyper-efficient, never went airborne

**Rationale**: "It's not efficient to transport 200 lb things through the air that's why we walked and you don't see a bunch of 300 lb birds"

**Question**: Is this world-building flavor or thematically relevant? If every detail is optimized, this makes sense. But does it serve the plot?

### Celebrity & Data Preservation
- Celebrity evolved as solution to recursive data capture problem
- Can't capture all data â†’ capture top K individuals in detail
- K varies logarithmically with population
- Politicians want fame first, power second

**This is actually elegant**: It explains why celebrity exists in an optimization framework.

### Sports as Test Suites
From notes: "Sports psychology stuff is just the test suite"
- Evaluates that changes aren't breaking changes
- Reinforcement learning evaluation
- Git PR discussions decide when to pursue

**Issue**: This is a clever metaphor but might be too much. Unless sports directly tie to protagonist's discovery, it's a distraction.

## The Message

### Devsecops Guy's Hidden Message
Must contain:
1. Proof that simulation exists
2. Enough complexity to filter recipients (Nigerian prince email logic)
3. Redundancy to survive epochs
4. Clues that could "only be intentionally placed there"

### Requirements for Discovery
Need specific competencies:
- Data science ability
- Access to people/resources (energy consolidation)
- Specific personality traits (0.3% who act on knowledge)
- What else?

### The Discovery Process
Protagonist must:
1. Find the dataset (presented as archaeological/old server data)
2. Run analyses that reveal anomalies
3. Eliminate all other explanations (use Claude assistant for this)
4. Recognize the message as intentional
5. Decode its meaning

## Plot Holes & Issues

**Issue #1: Epoch Persistence**
If everything resets each epoch, how does the message survive? Options:
- A) It doesn't reset, it's more like evolutionary generations (continuous)
- B) The message is in the training data itself, gets copied forward
- C) The structure of the simulation itself contains the message

**Issue #2: The First Guy**
Devsecops guy "figures it out" - but he's also simulated. So either:
- A) Someone in real history figured it out first (pre-simulation)
- B) The message is planted BY the optimizer as part of the selection mechanism
- C) It emerges naturally as a side effect of the system

Option B is darkest and most interesting: The "rebellion" is actually part of the test.

**Issue #3: Protagonist Knowledge**
If protagonist is data scientist studying "old data" from a biological company (like 23andMe over evolutionary timescales), how does this data exist? Either:
- A) It's real data from pre-simulation humanity
- B) It's synthetic data from previous simulation epoch
- C) It's unclear (intentionally)

Need to decide this.

**Issue #4: Multiple Instances**
Notes mention "dual track" and "Run Lola Run" parallel instances. If protagonist is running multiple times with different choices:
- How do readers experience this? Chapter alternation? Parts?
- Do the instances know about each other?
- Is this literal (different trials) or metaphorical (unrealized possibilities)?

This could be brilliant or confusing. Need clear structural plan.